{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d9b1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as  pd\n",
    "from pprint import pprint# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel# spaCy for preprocessing\n",
    "import spacy# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412080de",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "\n",
    "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c39286",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21db8ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc0bcc",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d0ec78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32a798c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f554cb3",
   "metadata": {},
   "source": [
    "### Remove emails and newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "745d1ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:6: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:4: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:6: DeprecationWarning: invalid escape sequence \\s\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5448\\2117143445.py:4: DeprecationWarning: invalid escape sequence \\S\n",
      "  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5448\\2117143445.py:6: DeprecationWarning: invalid escape sequence \\s\n",
      "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: 15 I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/ early 70s. It was called a Bricklin. The doors were really small. In addition, the front bumper was separate from the rest of the body. This is all I know. If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e-mail. Thanks, - IL ---- brought to you by your neighborhood Lerxst ---- ']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list \n",
    "data = data_raw.content.values.tolist()  \n",
    "# Remove Emails \n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]  \n",
    "# Remove new line characters \n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]  \n",
    "# Remove distracting single quotes \n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]  \n",
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e50613",
   "metadata": {},
   "source": [
    "### Tokenize words and cleanup the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa05963e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))            #deacc=True removes punctuations\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e130985",
   "metadata": {},
   "source": [
    "### Creating Bigram and Trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3020eb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp_posting_host', 'rac_wam_umd_edu', 'organization', 'university', 'of', 'maryland_college_park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front_bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e067dd",
   "metadata": {},
   "source": [
    "### Remove Stopwords, make bigrams and lemmatize\n",
    "Using lemmatization instead of stemming is a practice which especially pays off in topic modeling because lemmatized words tend to be more human-readable than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbb6e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1feb0e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['where', 's', 'thing', 'car', 'nntp_poste', 'host', 'rac_wam', 'umd', 'organization', 'university', 'park', 'line', 'wonder', 'enlighten', 'car', 'saw', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570792f",
   "metadata": {},
   "source": [
    "### Create Dictionary and Corpus needed for Topic Modeling\n",
    "Make sure to check if dictionary[id2word] or corpus is clean otherwise you may not get good quality topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "029db7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 5), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 2), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary \n",
    "id2word = corpora.Dictionary(data_lemmatized)  \n",
    "# Create Corpus \n",
    "texts = data_lemmatized  \n",
    "# Term Document Frequency \n",
    "corpus = [id2word.doc2bow(text) for text in texts]  \n",
    "# View \n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed439c",
   "metadata": {},
   "source": [
    "Gensim creates unique id for each word in the document. Its mapping of word_id and word_frequency. Example: (8,2) above indicates, word_id 8 occurs twice in the document and so on.\n",
    "\n",
    "This is used as input to LDA model.\n",
    "\n",
    "If you want to see what word corresponds to a given id, then pass the id as a key to dictionary. Example: id2word[4].\n",
    "\n",
    "Readable format of corpus can be obtained by executing below code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "673d70f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('addition', 1),\n",
       "  ('body', 1),\n",
       "  ('bricklin', 1),\n",
       "  ('bring', 1),\n",
       "  ('call', 1),\n",
       "  ('car', 5),\n",
       "  ('day', 1),\n",
       "  ('door', 2),\n",
       "  ('early', 1),\n",
       "  ('engine', 1),\n",
       "  ('enlighten', 1),\n",
       "  ('funky', 1),\n",
       "  ('history', 1),\n",
       "  ('host', 1),\n",
       "  ('info', 1),\n",
       "  ('know', 1),\n",
       "  ('late', 1),\n",
       "  ('lerxst', 1),\n",
       "  ('line', 1),\n",
       "  ('look', 2),\n",
       "  ('mail', 1),\n",
       "  ('make', 1),\n",
       "  ('model', 1),\n",
       "  ('name', 1),\n",
       "  ('neighborhood', 1),\n",
       "  ('nntp_poste', 1),\n",
       "  ('organization', 1),\n",
       "  ('park', 1),\n",
       "  ('production', 1),\n",
       "  ('rac_wam', 1),\n",
       "  ('really', 1),\n",
       "  ('rest', 1),\n",
       "  ('s', 1),\n",
       "  ('saw', 1),\n",
       "  ('separate', 1),\n",
       "  ('small', 1),\n",
       "  ('spec', 1),\n",
       "  ('sport', 1),\n",
       "  ('tellme', 1),\n",
       "  ('thank', 1),\n",
       "  ('thing', 1),\n",
       "  ('umd', 1),\n",
       "  ('university', 1),\n",
       "  ('where', 1),\n",
       "  ('wonder', 1),\n",
       "  ('year', 1)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a91200",
   "metadata": {},
   "source": [
    "### Building topic model\n",
    "> Parameters of LDA\n",
    "\n",
    "`Alpha and Beta are Hyperparameters` — alpha represents document-topic density and Beta represents topic-word density, \n",
    "\n",
    "`chunksize` is the number of documents to be used in each training chunk\n",
    "\n",
    "`update_every` determines how often the model parameters should be updated \n",
    "\n",
    "`passes` is the total number of training passes.\n",
    "\n",
    "A measure for best number of topics really depends on kind of corpus you are using, the size of corpus, number of topics you expect to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8b0b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe55510",
   "metadata": {},
   "source": [
    "### View topics in LDA model\n",
    "\n",
    "Each topic is combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "You can see keywords for each topic and weightage of each keyword using `lda_model.print_topics().`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b2e8cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.587*\"ax\" + 0.097*\"_\" + 0.047*\"max\" + 0.020*\"screen\" + 0.013*\"mouse\" + '\n",
      "  '0.008*\"mr\" + 0.007*\"modem\" + 0.006*\"el\" + 0.005*\"cds\" + 0.005*\"honda\"'),\n",
      " (1,\n",
      "  '0.080*\"key\" + 0.041*\"chip\" + 0.022*\"encryption\" + 0.021*\"server\" + '\n",
      "  '0.019*\"technology\" + 0.019*\"security\" + 0.018*\"clipper\" + 0.016*\"algorithm\" '\n",
      "  '+ 0.016*\"controller\" + 0.015*\"system\"'),\n",
      " (2,\n",
      "  '0.033*\"self\" + 0.028*\"normal\" + 0.024*\"son\" + 0.024*\"proof\" + '\n",
      "  '0.023*\"father\" + 0.022*\"alone\" + 0.021*\"muslim\" + 0.020*\"islam\" + '\n",
      "  '0.020*\"iran\" + 0.019*\"vote\"'),\n",
      " (3,\n",
      "  '0.033*\"people\" + 0.018*\"state\" + 0.015*\"right\" + 0.012*\"israel\" + '\n",
      "  '0.011*\"group\" + 0.011*\"government\" + 0.010*\"issue\" + 0.008*\"death\" + '\n",
      "  '0.008*\"kill\" + 0.008*\"case\"'),\n",
      " (4,\n",
      "  '0.030*\"trivial\" + 0.021*\"allen\" + 0.018*\"click\" + 0.016*\"ms_window\" + '\n",
      "  '0.004*\"rude\" + 0.000*\"sphere\" + 0.000*\"steven\" + 0.000*\"central\" + '\n",
      "  '0.000*\"hitachi\" + 0.000*\"circle\"'),\n",
      " (5,\n",
      "  '0.071*\"gun\" + 0.025*\"crime\" + 0.023*\"police\" + 0.022*\"weapon\" + '\n",
      "  '0.022*\"firearm\" + 0.019*\"safety\" + 0.019*\"rsa\" + 0.016*\"weight\" + '\n",
      "  '0.016*\"rate\" + 0.015*\"fire\"'),\n",
      " (6,\n",
      "  '0.087*\"law\" + 0.043*\"public\" + 0.036*\"suggest\" + 0.024*\"bill\" + '\n",
      "  '0.022*\"ride\" + 0.021*\"clinton\" + 0.020*\"private\" + 0.020*\"legal\" + '\n",
      "  '0.018*\"community\" + 0.016*\"ignore\"'),\n",
      " (7,\n",
      "  '0.037*\"report\" + 0.030*\"test\" + 0.026*\"patient\" + 0.021*\"study\" + '\n",
      "  '0.020*\"section\" + 0.017*\"publish\" + 0.016*\"review\" + 0.015*\"research\" + '\n",
      "  '0.014*\"april\" + 0.014*\"information\"'),\n",
      " (8,\n",
      "  '0.036*\"team\" + 0.036*\"game\" + 0.030*\"year\" + 0.025*\"win\" + 0.023*\"play\" + '\n",
      "  '0.017*\"player\" + 0.016*\"internet\" + 0.011*\"season\" + 0.011*\"division\" + '\n",
      "  '0.011*\"master\"'),\n",
      " (9,\n",
      "  '0.023*\"god\" + 0.021*\"say\" + 0.018*\"believe\" + 0.016*\"evidence\" + '\n",
      "  '0.016*\"reason\" + 0.013*\"christian\" + 0.011*\"claim\" + 0.011*\"man\" + '\n",
      "  '0.011*\"point\" + 0.010*\"think\"'),\n",
      " (10,\n",
      "  '0.094*\"video\" + 0.049*\"route\" + 0.049*\"mhz\" + 0.039*\"md\" + 0.029*\"zone\" + '\n",
      "  '0.022*\"km\" + 0.017*\"radius\" + 0.017*\"rs\" + 0.017*\"pop\" + '\n",
      "  '0.014*\"recommendation\"'),\n",
      " (11,\n",
      "  '0.100*\"apple\" + 0.043*\"multiple\" + 0.025*\"reflect\" + 0.010*\"fuzzy\" + '\n",
      "  '0.008*\"absurd\" + 0.005*\"judaism\" + 0.004*\"ibm_pc\" + 0.004*\"cart\" + '\n",
      "  '0.000*\"centris\" + 0.000*\"cd\"'),\n",
      " (12,\n",
      "  '0.053*\"armenian\" + 0.041*\"greek\" + 0.039*\"soldier\" + 0.039*\"village\" + '\n",
      "  '0.033*\"turk\" + 0.027*\"turkish\" + 0.023*\"occupy\" + 0.021*\"turkey\" + '\n",
      "  '0.013*\"escape\" + 0.012*\"armenia\"'),\n",
      " (13,\n",
      "  '0.060*\"car\" + 0.039*\"price\" + 0.039*\"sale\" + 0.036*\"sell\" + 0.027*\"model\" + '\n",
      "  '0.022*\"school\" + 0.022*\"buy\" + 0.021*\"distribution_na\" + '\n",
      "  '0.018*\"distribution\" + 0.017*\"corporation\"'),\n",
      " (14,\n",
      "  '0.021*\"system\" + 0.019*\"use\" + 0.016*\"window\" + 0.015*\"mail\" + '\n",
      "  '0.015*\"program\" + 0.014*\"file\" + 0.014*\"thank\" + 0.012*\"computer\" + '\n",
      "  '0.011*\"card\" + 0.011*\"line\"'),\n",
      " (15,\n",
      "  '0.030*\"line\" + 0.030*\"organization\" + 0.025*\"write\" + 0.024*\"do\" + '\n",
      "  '0.019*\"article\" + 0.018*\"get\" + 0.014*\"go\" + 0.013*\"know\" + '\n",
      "  '0.012*\"nntp_poste\" + 0.011*\"make\"'),\n",
      " (16,\n",
      "  '0.082*\"space\" + 0.025*\"pa\" + 0.023*\"earth\" + 0.020*\"launch\" + 0.019*\"moon\" '\n",
      "  '+ 0.018*\"nasa\" + 0.018*\"mission\" + 0.018*\"orbit\" + 0.015*\"satellite\" + '\n",
      "  '0.013*\"resource\"'),\n",
      " (17,\n",
      "  '0.079*\"physical\" + 0.062*\"pin\" + 0.041*\"character\" + 0.038*\"wing\" + '\n",
      "  '0.035*\"motif\" + 0.029*\"medium\" + 0.026*\"processor\" + 0.025*\"richard\" + '\n",
      "  '0.015*\"insert\" + 0.015*\"kevin\"'),\n",
      " (18,\n",
      "  '0.077*\"david\" + 0.068*\"graphic\" + 0.060*\"display\" + 0.039*\"paul\" + '\n",
      "  '0.030*\"texas\" + 0.022*\"workstation\" + 0.019*\"wave\" + 0.017*\"russia\" + '\n",
      "  '0.016*\"bar\" + 0.015*\"consequence\"'),\n",
      " (19,\n",
      "  '0.250*\"drive\" + 0.065*\"scsi\" + 0.042*\"ide\" + 0.029*\"circuit\" + 0.024*\"mb\" + '\n",
      "  '0.019*\"device\" + 0.017*\"quadra\" + 0.014*\"dale\" + 0.014*\"stream\" + '\n",
      "  '0.014*\"interface\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the keyword of topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed54dcc",
   "metadata": {},
   "source": [
    "You can see the top keywords and weights associated with keywords contributing to topic.\n",
    "\n",
    "Topics are words with highest probability in topic and the numbers are the probabilities of words appearing in topic distribution.\n",
    "\n",
    "But looking at keywords can you guess what the topic is?\n",
    "\n",
    "You may summarize topic-4 as space(In the above figure). Each one may have different topic at particular number , topic 4 might not be in the same place where it is now, it may be in topic 10 or any number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435279b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d93e8a62",
   "metadata": {},
   "source": [
    "### Evaluate topic models\n",
    "#### Compute model Perplexity and Coherence score\n",
    "Coherence score and perplexity provide a convinent way to measure how good a given topic model is.\n",
    "\n",
    "Our model will be better if the words in a topic are similar, so we will use topic coherence to evaluate our model. Topic coherence evaluates a single topic by measuring the degree of semantic similarity between high scoring words in the topic. A good model will generate topics with high topic coherence scores.\n",
    "\n",
    "### Perplexity\n",
    "perplexity metric as measuring how probable some new unseen data is given the model that was learned earlier. That is to say, how well does the model represent or reproduce the statistics of the held-out data.\n",
    "\n",
    "### Coherence Measures\n",
    "\n",
    "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic.\n",
    "\n",
    "Let’s take quick look at different coherence measures, and how they are calculated:\n",
    "\n",
    "`C_v` measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
    "\n",
    "`C_p` is based on a sliding window, one-preceding segmentation of the top words and the confirmation measure of Fitelson’s coherence\n",
    "\n",
    "`C_uci measure` is based on a sliding window and the pointwise mutual information (PMI) of all word pairs of the given top words\n",
    "\n",
    "`C_umass` is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure\n",
    "\n",
    "`C_npmi` is an enhanced version of the C_uci coherence using the normalized pointwise mutual information (NPMI)\n",
    "\n",
    "`C_a` is baseed on a context window, a pairwise comparison of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b361b31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69237aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -14.27292454068808\n",
      "\n",
      "Coherence Score:  0.5061935618822515\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "# a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd548fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7aaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58a20eca",
   "metadata": {},
   "source": [
    "### Visualize the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15b15d1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prepare() missing 2 required positional arguments: 'vocab' and 'term_frequency'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize the topics\u001b[39;00m\n\u001b[0;32m      2\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39menable_notebook()\n\u001b[1;32m----> 3\u001b[0m vis \u001b[38;5;241m=\u001b[39m \u001b[43mpyLDAvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2word\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m vis\n",
      "\u001b[1;31mTypeError\u001b[0m: prepare() missing 2 required positional arguments: 'vocab' and 'term_frequency'"
     ]
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd6d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               gensim.utils.ClippedCorpus(corpus, num_of_docs*0.75), \n",
    "               corpus]\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=540)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
